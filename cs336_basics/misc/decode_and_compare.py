import numpy as np
import pickle
from cs336_basics.tokenizer_class import Tokenizer

if __name__ == "__main__":
    # è®€å– vocab å’Œ merges
    vocab_pkl = "/mnt/disk3/yusheng/assignment1-basics/data/tokenizer/vocab_10000.pkl"
    merges_pkl = "/mnt/disk3/yusheng/assignment1-basics/data/tokenizer/merges_10000.pkl"

    with open(vocab_pkl, "rb") as f:
        vocab = pickle.load(f)

    with open(merges_pkl, "rb") as f:
        merges = pickle.load(f)

    tokenizer = Tokenizer(vocab, merges)

    # è¼‰å…¥ token ID çš„ .npy æª”
    input_token_ids_path = "/mnt/disk3/yusheng/assignment1-basics/data/tokenizer/valid_token_ids_10000.npy"
    output_text_path = "/mnt/disk3/yusheng/assignment1-basics/data/tokenizer/decoded_valid_text.txt"

    token_ids = np.load(input_token_ids_path)

    # é‚„åŸæˆæ–‡å­—
    decoded_text = tokenizer.decode(token_ids.tolist())

    # å„²å­˜ç‚º .txt
    with open(output_text_path, "w", encoding="utf-8") as f:
        f.write(decoded_text)

    print(f"âœ… å·²å„²å­˜é‚„åŸæ–‡å­—åˆ°: {output_text_path}")

    # åŸå§‹æ–‡å­—æª”æ¡ˆè·¯å¾‘
    origin_text_path = "/mnt/disk3/yusheng/assignment1-basics/data/TinyStoriesV2-GPT4-valid.txt"

    # æ¯”è¼ƒæ•´é«”å…§å®¹æ˜¯å¦ä¸€è‡´
    print("\nğŸ” æ­£åœ¨æ¯”å°æ•´é«”å…§å®¹æ˜¯å¦ä¸€è‡´...")
    with open(origin_text_path, "r", encoding="utf-8") as f1, open(output_text_path, "r", encoding="utf-8") as f2:
        origin_all = f1.read().strip()
        decoded_all = f2.read().strip()

        if origin_all == decoded_all:
            print("ğŸ‰ å…©å€‹æª”æ¡ˆå…§å®¹å®Œå…¨ä¸€è‡´ï¼")
        else:
            print("âŒ å…©å€‹æª”æ¡ˆå…§å®¹ä¸ä¸€è‡´ã€‚")
